= NASA Server Log Analytics application

== Introduction
Security breaches happen. And when they do, your server logs may be your best line of defense. We take the server-log analysis to the next level by speeding and improving security forensics and providing a low cost platform to show compliance. The dataset which we are going to use in this demo is of NASA-HTTP. It has HTTP requests to the NASA Kennedy Space Center WWW server in Florida. 

image::images/front.png[width=800]

=== Before you start
. Everything is Case-Sensitive.
. Check all your connections and spellings through the workshop


=== Pre-requisites

* Laptop with a supported OS (Windows 7 not supported).
* A modern browser like Google Chrome or Firefox (IE not supported).

=== Connecting to your cluster

. *Please open the IP in your browser (shown on the main screen) and finish the registration! Use the registration code shown on the main screen* 

. You will be asked to modify the password, *this password will be only used to access this registration site* if you accidentally close it. In this case use *Login* on this site. 

. This registration process will open the firewall to your specific IP and assign you to one of the cluster nodes automatically. *Take note of your cluster ip address to avoid any further issues* 

. Login into *Cloudera Manager* and familiarize yourself with the services installed. Below a screenshot of Chrome open with the main page after the registration, to open Cloudera Manager click on the following:

NOTE: During the workshop *you will not use the previously selected password on the registration web page*, please *copy the username and password shown on the main screen* for the Cloudera Services. 

NOTE: The credentials are case sensitive, the password has capital "S". 

*Screenshots for the previous steps:*

Below a screenshot of Chrome open with the main page after the registration, to open Cloudera Manager click on the following: 

image::images/openclouderamanager.png[width=800]

On the Cloudera Manager UI, enter the password for the Cloudera Services: 

image::images/logintoclouderamanager.png[width=800]

if you succesfuly login you will see the components of the cluster and current health, also you can manage your cluster setup from this single control pane: 

image::images/clouderamanager.png[width=800]

NOTE: The necessary files have been preloaded to the machine which is running the Cloudera Private Cloud Base one node cluster to the /nasademo/NASALogs/NASA_access_log_Aug95. 
This could be directly fetched from the hosts as well, also we can use minifi agents on the servers to collect and also prepare the raw log files and then send it to nifi, however in this short workshop the intention is to showcase the capabilities of nifi and creating a dashboard to gather insights, logs collection from edge devices is not part of this session. 

== Steps of the workshop
. 

= WORKSHOP START
[[lab_1, Lab 1]]
== Lab 1 - Registering a schema in Schema Registry

The data produced by the web server is described by the schema in file `link:https://raw.githubusercontent.com/simontarzi/nasa-log-demo/main/log.avsc[log.avsc]`. In this lab we will register this schema in Schema Registry so that our flows in NiFi can refer to schema using an unified service. This will also allow us to evolve the schema in the future, if needed, keeping older versions under version control, so that existing flows and flowfiles will continue to work.

. Go to the following URL, which contains the schema definition we'll use for this lab. Select all contents of the page and copy it.
+
`link:https://raw.githubusercontent.com/simontarzi/nasa-log-demo/main/log.avsc, window="_blank"]`

. In the Schema Registry Web UI, click the `+` sign to register a new schema.

. Click on a blank area in the *Schema Text* field and paste the contents you copied.

. Complete the schema creation by filling the following properties and save the schema.
+
[source,yaml]
----
Name:          LogReading
Description:   Schema for the data generated by the web servers
Type:          Avro schema provider
Schema Group:  Kafka
Compatibility: Backward
Evolve:        checked
----
+
image::images/register_schema.png[width=800]




[source,shell]
----
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----







== Steps

. Create /nasademo
. chmod -R 777 nasademo
. Put NASALogs folder to /nasademo
. Put GeoLite2-City.mmdb to /nasademo/GeoLite2-City.mmdb

Logs will be put into /nifi folder automatically

For Zeppelin be sure to run these

On 7.1.9: 

[source,shell]
----
cp /opt/cloudera/parcels/CDH-7.1.9-1.cdh7.1.9.p0.44702451/jars/hive-jdbc-* ./opt/cloudera/parcels/CDH-7.1.9-1.cdh7.1.9.p0.44702451/lib/zeppelin/interpreter/jdbc/

cp /opt/cloudera/parcels/CDH-7.1.9-1.cdh7.1.9.p0.44702451/lib/hadoop/client/*.jar ./opt/cloudera/parcels/CDH-7.1.9-1.cdh7.1.9.p0.44702451/lib/zeppelin/interpreter/jdbc/

----

On 7.1.8: 

[source,shell]
----
cp /opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p0.30990532/jars/hive-jdbc-* ./opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p0.30990532/lib/zeppelin/interpreter/jdbc/

cp /opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p0.30990532/lib/hadoop/client/*.jar ./opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p0.30990532/lib/zeppelin/interpreter/jdbc/

----

In cloudera manager search for *Zeppelin Shiro Urls Block* on the Configuration tab, and locate the following role: 

[source,json]
----
/api/interpreter/** =
----

and change it from: "authc, roles[{{zeppelin_admin_group}}]" to  *authc, roles[admin]*

so the end result will be: 

[source,json]
----
/api/interpreter/** = authc, roles[admin]
----


In zeppelin go to the upper right corner to user, and select interpreter, add a new interpreter with the name: Hive and group:jdbc, set the following:

[source,json]
----
default.driver = org.apache.hive.jdbc.HiveDriver
default.url = jdbc:hive2://localhost:10000/
----
